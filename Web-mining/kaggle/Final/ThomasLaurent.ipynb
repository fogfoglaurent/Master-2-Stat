{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#  Readme #\n",
    "###########\n",
    "# Ce programme fonctionne de la maniere suivant\n",
    "#1) Les donnees training des mails sont importees ainsi que les donnees des destinataires pour chaque mail\n",
    "#et les jeux de donnees sont joints\n",
    "#2) Pour creer un graphe directionnel pondere par le nombre de mails entre expediteur et destinataire,\n",
    "#le nombre de mail d'un expediteur vers un destinataire est calcule, puis le graphe est realise\n",
    "#3) Le lien entre chaque expediteur et destinataire est mesure par le coefficient Jaccard et Adamic\n",
    "#4) Pour les feature text, on procede a une tokenisation en enlevant d'avoir les chaines de caracteres \n",
    "# avec des caracteres numeriques ou speciaux, et en retirant certains mots non pertinent\n",
    "#, puis on procede a une racinisation/stemming.\n",
    "#5) On extrait la matrice de comptage des mots puis on estime un modele LDA avec 30 topics\n",
    "#6) Pour chaque destinataire, on calcule le centroid des topics des mails recus d'un expediteur particulier\n",
    "#a partir des distributions\n",
    "#obtenus a partir du modele LDA\n",
    "#7) Pour chaque mail, on calcule la distance au cosinus entre les topics du mail par rapport \n",
    "#au centroid expediteur-destinataire possibles\n",
    "#8) Ensuite, on fait une concatenation pour obtenir un jeu de donnees avec les features text et reseau.\n",
    "#9) Le jeu de donnees est equilibre pour obtenir 50% de positifs/destinataires reels et 50% de negatif\n",
    "#10) On estime un modele RandomForest pour classifier une paire mail-destinataire de maniere binaire,\n",
    "#egale a 1 si destinataire, 0 sinon\n",
    "#11) Les donnees test sont importees et joints de la meme maniere que pour le jeu de donnees training\n",
    "#12) Les feature reseaux obtenus sur le jeu d'apprentissage sont ajoutes au jeu de donnees par jointure\n",
    "#13) Les feature texte sont obtenus en utilisant le modele LDA estime sur l'echantillon d'apprentissage \n",
    "#pour extraire les topics des mails test et en calculant la distance au cosinus avec chaque\n",
    "#centroid expediteur-destinataire de l'echantillon d'apprentissage\n",
    "#14) Enfin les scores sont obtenus en utilisant le modele RandomForest appris et pour chaque mail, les scores\n",
    "#sont ranges dans l'ordre decroissant et les 10 premiers destinataires sont retournes dans un fichier csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import date\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "##Data Import##\n",
    "training_info=pd.read_csv(\"/Users/thomaslaurent/Documents/Cours-M2/Web-mining/kaggle/training_info_sid.csv\",parse_dates=True,header=None)\n",
    "training_set=pd.read_csv(\"/Users/thomaslaurent/Documents/Cours-M2/Web-mining/kaggle/training_set_sid.csv\",header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#Training data import and reshaping#\n",
    "####################################\n",
    "\n",
    "##Data reshaping for training_info##\n",
    "\n",
    "\n",
    "#Changing column names for imported data#\n",
    "training_info.columns=['Mail_id','Dates','Contents','Recipients']\n",
    "training_set.columns=['Sender','Mail_id']\n",
    "\n",
    "training_info['Dates']=pd.to_datetime(training_info['Dates'])\n",
    "training_info['Dates']=training_info['Dates'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "#Changing recipient variable into list#\n",
    "training_info['Recipients']=training_info['Recipients'].str.split(\" \")\n",
    "\n",
    "##Data reshaping for training_set##\n",
    "\n",
    "#Changing Mail_id variable into list#\n",
    "training_set['Mail_id']=training_set['Mail_id'].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a function for transposing list variable to rows - one element of the list by row##\n",
    "def explode(df, lst_cols, fill_value=''):\n",
    "    # make sure lst_cols is a list\n",
    "    if lst_cols and not isinstance(lst_cols, list):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except lst_cols\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "\n",
    "    if (lens > 0).all():\n",
    "        # ALL lists in cells aren't empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .loc[:, df.columns]\n",
    "    else:\n",
    "        # at least one list in cells is empty\n",
    "        return pd.DataFrame({\n",
    "            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n",
    "            for col in idx_cols\n",
    "        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n",
    "          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n",
    "          .loc[:, df.columns]\n",
    "\n",
    "##Transposing each mail_id and recipient to row ##\n",
    "dataset_recipient_mail=explode(training_set,[\"Mail_id\"])\n",
    "training_info_mail=explode(training_info,[\"Recipients\"])\n",
    "\n",
    "##Merge sender and recipient info for each mail##\n",
    "dataset_recipient_mail[\"Mail_id\"]=pd.to_numeric(dataset_recipient_mail[\"Mail_id\"])\n",
    "merged_recipient_sender=pd.merge(dataset_recipient_mail,training_info_mail[[\"Mail_id\",\"Recipients\"]],on=\"Mail_id\",how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creation of contingency table to calculate weight based on the number of email exchanged between##\n",
    "##sender and recipient##\n",
    "df=pd.crosstab(merged_recipient_sender[\"Sender\"],merged_recipient_sender[\"Recipients\"])\n",
    "idx = df.columns.union(df.index)\n",
    "df = df.reindex(index = idx, columns=idx, fill_value=0)\n",
    "df = df.iloc[:,:].divide(df.iloc[:,:].sum(axis=0),axis=1)\n",
    "df=df.fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#Network feature extraction#\n",
    "############################\n",
    "\n",
    "##Creation of a directed graph between senders and recipients##\n",
    "##and calculation of Jaccard and Adamic/Adar coefficients##\n",
    "G = nx.from_pandas_adjacency(df,create_using=nx.DiGraph())\n",
    "g_copy = nx.Graph(G)\n",
    "\n",
    "##Calculating Jaccard and adamic coefficient for each edge##\n",
    "list_classe=[]\n",
    "list_combination=[]\n",
    "list_jaccard=[]\n",
    "list_adamic=[]\n",
    "for l in g_copy.nodes():\n",
    "    for j in g_copy.nodes():\n",
    "        if l != j :\n",
    "            list_combination.append((l,j))\n",
    "            lpJaccard= nx.jaccard_coefficient(g_copy,[(l,j)])\n",
    "            lpAdamic= nx.adamic_adar_index(g_copy,[(l,j)])\n",
    "            for u, v, p in lpJaccard:\n",
    "                value_jaccard=p\n",
    "                list_jaccard.append(value_jaccard)\n",
    "            for u, v, p in lpAdamic:\n",
    "                value_adamic=p\n",
    "                list_adamic.append(value_adamic)\n",
    "#Creating a list for existing edges#\n",
    "            if g_copy.has_edge(l,j) :\n",
    "                list_classe.append((l,j))\n",
    "\n",
    "#Creating a dataset class_nodes containing each coefficient\n",
    "#for each node and classe as 1 if the edge exists#\n",
    "class_nodes=pd.DataFrame(list_combination,columns=[\"Sender\",\"Recipient\"])\n",
    "class_nodes[\"Jaccard\"]=pd.Series(list_jaccard)\n",
    "class_nodes[\"Adamic\"]=pd.Series(list_adamic)\n",
    "class_nodes_existing=pd.DataFrame(list_classe,columns=[\"Sender\",\"Recipient\"])\n",
    "class_vector=pd.Series(np.ones(class_nodes_existing.shape[0]))\n",
    "class_nodes_existing[\"classe\"]=class_vector\n",
    "\n",
    "class_nodes=class_nodes.merge(class_nodes_existing,on=[\"Sender\",\"Recipient\"],how=\"left\").fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#Text feature extraction#\n",
    "#########################\n",
    "\n",
    "##Identifying topics for each mail for non void mail##\n",
    "token_df=training_info\n",
    "token_df = token_df[token_df['Contents'].notnull()]\n",
    "token_df=token_df.reset_index()\n",
    "\n",
    "#Creating stop_words list and extending the list with \"http\",\"AM\" and \"PM\"#\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"AM\",\"PM\",\"http\"])\n",
    "\n",
    "##Creating a function to select only alphanumeric words, removing stop words and stemming##\n",
    "def cleaning_words(content):\n",
    "    text = [word for word in nltk.word_tokenize(content)]\n",
    "    porter = PorterStemmer()\n",
    "    tokens_no_stop_word = []\n",
    "    filtered_tokens = []\n",
    "    stems=[]\n",
    "    for token in text:\n",
    "        if token.isalpha():\n",
    "            filtered_tokens.append(token)\n",
    "    tokens_no_stop_word=[w for w in filtered_tokens if w not in stop_words]\n",
    "    stems = [porter.stem(w) for w in tokens_no_stop_word]\n",
    "    return stems\n",
    "\n",
    "##LDA method ##\n",
    "n_features=5000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=2, max_features=n_features,tokenizer=cleaning_words)\n",
    "tf = tf_vectorizer.fit_transform(token_df[\"Contents\"])\n",
    "# Convert sparse matrix to gensim corpus\n",
    "import gensim\n",
    "corpus = gensim.matutils.Sparse2Corpus(tf, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words\n",
    "id_map = dict((v, k) for k, v in tf_vectorizer.vocabulary_.items())\n",
    "\n",
    "# LDA training using 30 topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=30, id2word=id_map, passes=25, random_state=34)\n",
    "\n",
    "#Creating a function to return a dataframe of topic distribution by document\n",
    "def lda_corpus(corpus):\n",
    "    lda_corpus = ldamodel[corpus]\n",
    "    lda=pd.DataFrame([])\n",
    "    list_topic=[]\n",
    "    dict_topics = {}\n",
    "    index=0\n",
    "    for doc in lda_corpus:\n",
    "        dict_topics[index]=dict(doc)\n",
    "        index=index+1\n",
    "    topic_doc_df=pd.DataFrame(dict_topics).fillna(0)\n",
    "    lda_df=topic_doc_df.T\n",
    "    return lda_df\n",
    "\n",
    "#Creating dataframe of topic distribution for document in the training data\n",
    "lda_df=lda_corpus(corpus)\n",
    "\n",
    "#Merging topic vector to original training_info dataframe#\n",
    "merged_info_lda=pd.concat([token_df,lda_df],axis=1)\n",
    "del merged_info_lda[\"Contents\"]\n",
    "del merged_info_lda[\"Recipients\"]\n",
    "del merged_info_lda[\"Dates\"]\n",
    "\n",
    "original_info_lda=pd.merge(training_info_mail,merged_info_lda,on=[\"Mail_id\"],how=\"left\").fillna(0)\n",
    "del original_info_lda[\"index\"]\n",
    "del original_info_lda[\"Contents\"]\n",
    "\n",
    "#Mapping senders to original_info_lda dataframe#\n",
    "original_info_lda=pd.merge(original_info_lda,dataset_recipient_mail,on=[\"Mail_id\"],how=\"left\")\n",
    "\n",
    "#Merging clustering features to original_info_lda dataframe#\n",
    "original_info_lda=pd.merge(original_info_lda,class_nodes,left_on=[\"Sender\",\"Recipients\"],right_on=[\"Sender\",\"Recipient\"],how=\"left\")\n",
    "del original_info_lda[\"Recipients\"]\n",
    "del original_info_lda[\"Dates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculating topic similarities##\n",
    "\n",
    "#Calculate centroid of topics for each recipient#\n",
    "original_info_lda.fillna(0)\n",
    "listvar=[\"Sender\",\"Mail_id\"]\n",
    "listvar.extend(range(0,30))\n",
    "LDA_by_recipient=original_info_lda[listvar]\n",
    "LDA_by_recipient=pd.merge(LDA_by_recipient,merged_recipient_sender[[\"Mail_id\",\"Recipients\"]],left_on=\"Mail_id\",right_on=\"Mail_id\",how=\"left\")\n",
    "LDA_by_recipient=LDA_by_recipient.groupby([\"Recipients\",\"Sender\"]).sum()\n",
    "LDA_by_recipient=LDA_by_recipient.rename(index=str,columns={\"Recipients\":\"Recipient\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Creating the function to calculate cosine similarity\n",
    "def cosine_recipient(dataframe):\n",
    "    listvar=list(range(0,30))\n",
    "    cosine_LDA_mails=cosine_similarity(dataframe[listvar],LDA_by_recipient[listvar])\n",
    "    cosine_LDA_mails=pd.DataFrame(cosine_LDA_mails)\n",
    "    cosine_LDA_mails.columns=list(LDA_by_recipient.index)\n",
    "    cosine_LDA_mails=cosine_LDA_mails.reset_index()\n",
    "    cosine_LDA_mails[\"Mail_id\"]=dataframe[\"Mail_id\"]\n",
    "    cosine_LDA_mails[\"Senders\"]=dataframe[\"Sender\"]\n",
    "    cosine_LDA_mails=cosine_LDA_mails.reset_index().set_index([\"Mail_id\",\"Senders\"])\n",
    "    cosine_LDA_mails=pd.DataFrame(cosine_LDA_mails.stack()).reset_index()\n",
    "    cosine_LDA_mails.columns=[\"Mail_id\",\"Senders\",\"Recipient\",\"Cosine\"]\n",
    "    LDA_by_recipient_copy=LDA_by_recipient.copy()\n",
    "    LDA_by_recipient_copy[\"index_var\"]=LDA_by_recipient_copy.index\n",
    "    cosine_LDA_mails=cosine_LDA_mails.merge(LDA_by_recipient_copy[[\"index_var\",\"Sender\",\"Recipients\"]],left_on=[\"Recipient\"],right_on=[\"index_var\"],how=\"inner\")\n",
    "    cosine_LDA_mails=cosine_LDA_mails[cosine_LDA_mails.Senders==cosine_LDA_mails.Sender]\n",
    "    cosine_LDA_mails=cosine_LDA_mails[[\"Mail_id\",\"Sender\",\"Recipients\",\"Cosine\"]]\n",
    "    cosine_LDA_mails=cosine_LDA_mails.rename(columns={\"Recipients\":\"Recipient\"})\n",
    "    return cosine_LDA_mails\n",
    "\n",
    "#Calculating cosine similarities for the train dataframe\n",
    "cosine_LDA_mails=cosine_recipient(original_info_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Data reshaping #\n",
    "##################\n",
    "\n",
    "##Creating a dataframe for each email and each possible recipient##\n",
    "Mail_analysis=cosine_LDA_mails.merge(class_nodes,left_on=[\"Sender\",\"Recipient\"],right_on=[\"Sender\",\"Recipient\"],how=\"inner\")\n",
    "del Mail_analysis[\"classe\"]\n",
    "\n",
    "#Creating class coded as 1 for real recipients#\n",
    "training_info_class=training_info_mail[[\"Mail_id\",\"Recipients\"]]\n",
    "training_info_class[\"classe\"]=pd.Series(np.ones(training_info_class.shape[0]))\n",
    "Mail_analysis=Mail_analysis.rename(index=str, columns={\"Recipient\": \"Recipients\"})\n",
    "Mail_analysis=Mail_analysis.merge(training_info_class,left_on=[\"Mail_id\",\"Recipients\"],right_on=[\"Mail_id\",\"Recipients\"],how=\"left\")\n",
    "Mail_analysis[\"classe\"]=Mail_analysis[\"classe\"].fillna(0)\n",
    "\n",
    "#Removing data where recipient is identical to sender#\n",
    "Mail_analysis=Mail_analysis[Mail_analysis[\"Recipients\"]!=Mail_analysis[\"Sender\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating dataset for model training##\n",
    "#Balancing negative and positive class 50%#\n",
    "Positive_set=Mail_analysis[Mail_analysis[\"classe\"]==1]\n",
    "length_positive=Positive_set.shape[0]\n",
    "Negative_set=Mail_analysis[Mail_analysis[\"classe\"]==0].sample(length_positive)\n",
    "\n",
    "train_set=pd.concat([Positive_set,Negative_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################\n",
    "# Training the model  #\n",
    "#######################\n",
    "\n",
    "#Classification using random forest#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = [\"Cosine\",\"Adamic\",\"Jaccard\"]\n",
    "y = train_set[\"classe\"]\n",
    "X = train_set[features]\n",
    "\n",
    "# Create a random forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "\n",
    "# Training the classifier\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#  Importing test data   #\n",
    "##########################\n",
    "\n",
    "\n",
    "#Importing test datasets\n",
    "test_info=pd.read_csv(\"/Users/thomaslaurent/Documents/Cours-M2/Web-mining/kaggle/test_info_sid.csv\",parse_dates=True,header=None)\n",
    "test_set=pd.read_csv(\"/Users/thomaslaurent/Documents/Cours-M2/Web-mining/kaggle/test_set_sid.csv\",header=None)\n",
    "\n",
    "##Data reshaping for test_info##\n",
    "\n",
    "#Changing column names of datasets#\n",
    "test_info.columns=['Mail_id','Dates','Contents']\n",
    "test_set.columns=['Sender','Mail_id']\n",
    "\n",
    "#Changing Mail_id variable into list#\n",
    "test_set['Mail_id']=test_set['Mail_id'].str.split(\" \")\n",
    "test_recipient_mail=explode(test_set,[\"Mail_id\"])\n",
    "\n",
    "test_recipient_mail[\"Mail_id\"]=pd.to_numeric(test_recipient_mail[\"Mail_id\"])\n",
    "\n",
    "#######################################\n",
    "#  Adding network and text features   #\n",
    "#######################################\n",
    "\n",
    "#Adding network features to test dataset\n",
    "test_network_feature=test_recipient_mail.merge(class_nodes,left_on=[\"Sender\"],right_on=[\"Sender\"],how=\"left\")\n",
    "\n",
    "##Identifying topics for each mail for non void mail##\n",
    "token_df=test_info\n",
    "token_df = token_df[token_df['Contents'].notnull()]\n",
    "token_df=token_df.reset_index()\n",
    "\n",
    "# Create the matrix of token counts for test dataset\n",
    "X = tf_vectorizer.transform(token_df[\"Contents\"])\n",
    "\n",
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Generating topic distribution by mail in the test dataset\n",
    "lda_df=lda_corpus(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging topic vector to original training_info dataframe#\n",
    "test_info_lda=pd.concat([token_df,lda_df],axis=1)\n",
    "del test_info_lda[\"Contents\"]\n",
    "del test_info_lda[\"Dates\"]\n",
    "del test_info_lda[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping senders to topics dataframe#\n",
    "test_info_lda=test_info_lda.merge(test_recipient_mail,on=\"Mail_id\",how=\"left\")\n",
    "\n",
    "#Calculating cosine similarities for each email content to the centroid of each potential recipient\n",
    "#in the test dataset\n",
    "cosine_LDA_mails=cosine_recipient(test_info_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine network and text features#\n",
    "test_dataset=pd.merge(test_network_feature[[\"Mail_id\",\"Jaccard\",\"Adamic\",\"classe\",\"Recipient\"]],cosine_LDA_mails,left_on=[\"Mail_id\",\"Recipient\"],right_on=[\"Mail_id\",\"Recipient\"],how=\"inner\")\n",
    "\n",
    "#Data handling for email without content-using only feature network for all possible pairs\n",
    "notextfeat=test_recipient_mail[-test_recipient_mail[\"Mail_id\"].isin(cosine_LDA_mails[\"Mail_id\"].unique())]\n",
    "notextfeat=notextfeat.merge(test_network_feature[[\"Jaccard\",\"Adamic\",\"classe\",\"Sender\",\"Recipient\"]],left_on=\"Sender\",right_on=\"Sender\",how=\"inner\")\n",
    "notextfeat[\"Mail_id\"].unique()\n",
    "test_dataset=pd.concat([test_dataset,notextfeat])\n",
    "\n",
    "#Replacing missing values by 0#\n",
    "test_dataset=test_dataset.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#  Predicted the score for each mail and recipient pair  #\n",
    "##########################################################\n",
    "\n",
    "#Predicting potential recipients using trained Random Forest model#\n",
    "features = [\"Cosine\",\"Adamic\",\"Jaccard\"]\n",
    "X = test_dataset[features]\n",
    "predictions=clf.predict_proba(X)\n",
    "\n",
    "#Adding score of being recipient to the test dataset#\n",
    "predictions=pd.DataFrame(predictions)\n",
    "predictions=predictions[1]\n",
    "test_dataset=test_dataset.reset_index()\n",
    "predictions=predictions.reset_index()\n",
    "predicted_set=pd.concat([test_dataset,predictions],axis=1)\n",
    "predicted_set=predicted_set.rename(index=str, columns={1: \"Score\"})\n",
    "\n",
    "#Ranking prediction by decreasing score by email and keeping the 10 first recipients#\n",
    "predicted_list=predicted_set[[\"Mail_id\",\"Recipient\",\"Score\"]].sort_values([\"Mail_id\", \"Score\"], ascending=[True, False])\n",
    "predicted_list=predicted_list.groupby(\"Mail_id\").head(10).reset_index(drop=True)\n",
    "predicted_list=predicted_list[[\"Mail_id\",\"Recipient\"]]\n",
    "\n",
    "#Reshaping dataframe to output the list of the best 10 predicted recipients by email\n",
    "output_list=pd.DataFrame(predicted_list.groupby(\"Mail_id\").apply(lambda x: list(x.Recipient)))\n",
    "output_list=output_list.reset_index()\n",
    "output_list.columns=[\"Id\",\"Recipients\"]\n",
    "output_list[\"Recipients\"]=output_list.Recipients.str.join(\" \")\n",
    "\n",
    "#Saving results as a csv#\n",
    "output_list.to_csv(\"pred_recipients_Thomas_Laurent.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
